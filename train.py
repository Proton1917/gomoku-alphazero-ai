import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import random
from tqdm import tqdm
import multiprocessing
import matplotlib.pyplot as plt
import math
from functools import partial
import torch.nn.functional as F
import copy
import os
import time

board_size = 15
class Config:
    batch_size = 128  # å¢å¤§batch sizeæå‡è®­ç»ƒæ•ˆç‡ï¼ˆM4 Proå†…å­˜å……è¶³ï¼‰
    num_epochs = 5    # å¢åŠ æ¯è½®è®­ç»ƒçš„epochs
    learning_rate = 2e-4  # ç¨å¾®æé«˜å­¦ä¹ ç‡åŠ é€Ÿæ”¶æ•›
    train_ratio = 0.9
    num_samples = 200  # å¤§å¹…å¢åŠ æ ·æœ¬æ•°é‡ï¼ˆ10å€æå‡ï¼‰
    channel = 64      # å¢åŠ é€šé“æ•°æå‡æ¨¡å‹å®¹é‡ï¼ˆ2å€æå‡ï¼‰
    num_workers = 4   # MPSä¸‹ä¿æŒå•è¿›ç¨‹ä»¥é¿å…é—®é¢˜
    train_simulation = 50  # å¢åŠ MCTSæ¨¡æ‹Ÿæ¬¡æ•°æå‡æ•°æ®è´¨é‡
    base_path = None
    model_path = 'gomoku_cnn_strong'  # æ–°çš„å¼ºåŒ–è®­ç»ƒæ¨¡å‹è·¯å¾„
    mcts_type = 'mean'
    output_info = True
    collect_subnode = True
    train_buff = 0.8

class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
        self.relu = nn.ReLU()

    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual
        out = self.relu(out)
        return out

class ValueCNN(nn.Module):
    def __init__(self, in_channels=3, hidden_channels=64, num_blocks=8, value_dim=256):  # å¢åŠ æ·±åº¦å’Œå®¹é‡
        super(ValueCNN, self).__init__()
        
        self.conv_init = nn.Conv2d(in_channels, hidden_channels, kernel_size=3, padding=1)
        self.bn_init = nn.BatchNorm2d(hidden_channels)
        
        self.res_blocks = nn.ModuleList([
            ResidualBlock(hidden_channels) for _ in range(num_blocks)
        ])
        
        self.policy_conv1 = nn.Conv2d(hidden_channels, hidden_channels // 2, kernel_size=3, padding=1)
        self.policy_bn1 = nn.BatchNorm2d(hidden_channels // 2)
        self.policy_conv2 = nn.Conv2d(hidden_channels // 2, 1, kernel_size=3, padding=1)
        
        self.value_conv = nn.Conv2d(hidden_channels, 1, kernel_size=1)
        self.value_bn = nn.BatchNorm2d(1)
        self.value_fc1 = nn.Linear(board_size * board_size, value_dim)
        self.value_fc2 = nn.Linear(value_dim, 1)

    def forward(self, x):
        x = F.relu(self.bn_init(self.conv_init(x)))
        
        for block in self.res_blocks:
            x = block(x)
        
        policy = F.relu(self.policy_bn1(self.policy_conv1(x)))
        policy = self.policy_conv2(policy)
        policy = policy.squeeze(1)  
        policy_logits = policy.view(x.size(0), -1)
        
        value = F.relu(self.value_bn(self.value_conv(x)))
        value = value.view(x.size(0), -1)
        value = F.relu(self.value_fc1(value))
        value = torch.tanh(self.value_fc2(value)) 
        
        return value, policy_logits
    
    def calc(self, x):
        self.eval()
        with torch.no_grad():
            value, logits = self.forward(x)
            probs = F.softmax(logits, dim=1).view(-1, board_size, board_size)
            return value, probs

class Weighted_Dataset(Dataset):
    def __init__(self, boards, policies, values, weights):
        self.boards = boards
        self.policies = policies
        self.values = values
        self.weights = weights

    def __len__(self):
        return len(self.boards)

    def __getitem__(self, idx):
        return self.boards[idx], self.policies[idx], self.values[idx], self.weights[idx]


def board_to_tensor(board : list[list[int]]):
    """
    å°†board_sizexboard_sizeçš„æ£‹ç›˜è½¬æ¢ä¸º3é€šé“çš„tensor
    board: List[List[int]], 1ä»£è¡¨å½“å‰æ–¹, -1ä»£è¡¨å¯¹æ–¹, 0ä»£è¡¨ç©º
    è¿”å›: (3, board_size, board_size) tensor
    """
    board = np.array(list(board))
    
    # åˆ›å»º3ä¸ªé€šé“
    current_player = (board == 1).astype(np.float32)  # å½“å‰ç©å®¶çš„æ£‹å­
    opponent = (board == -1).astype(np.float32)       # å¯¹æ‰‹çš„æ£‹å­
    empty = (board == 0).astype(np.float32)           # ç©ºä½
    
    # å †å æˆ3é€šé“
    tensor = np.stack([current_player, opponent, empty], axis=0)
    return torch.FloatTensor(tensor)

def get_calc(model, board):
    if torch.cuda.is_available():
        device = torch.device('cuda')
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
    else:
        device = torch.device('cpu')
    
    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    model = model.to(device)
    board_tensor = board_to_tensor(board).unsqueeze(0).to(device)
    with torch.no_grad():
        value, policy = model.calc(board_tensor)
    return float(value), policy.squeeze(0).cpu().numpy().tolist()

def evaluation_func(board : list[list[int]]):
    num_used = 0
    for i in range(0, board_size):
        for j in range(0, board_size):
            if board[i][j] != 0:
                num_used += 1
    for i in range(0, board_size):
        for j in range(0, board_size):
            if board[i][j] != 0:
                for (x, y) in [(0, 1), (1, 0), (1, 1), (1, -1)]:
                    cnt = 0
                    for d in range(0, 5):
                        ni = i + d * x
                        nj = j + d * y
                        if 0 <= ni and ni < board_size and 0 <= nj and nj < board_size and board[i][j] == board[ni][nj]:
                            cnt += 1
                        else:
                            break
                    if cnt == 5:
                        if board[i][j] == 1:
                            return (1 - num_used * 3e-4)
                        else:
                            return -(1 - num_used * 3e-4)
    return 0

def generate_random_board(model):
    """ç”Ÿæˆéšæœºçš„æ£‹ç›˜çŠ¶æ€"""
    
    if torch.cuda.is_available():
        device = torch.device('cuda')
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
    else:
        device = torch.device('cpu')
    model = model.to(device)
    perm = []
    for i in range(0, board_size):
        for j in range(0, board_size):
            perm.append((i, j))
    random.shuffle(perm)

    best_val = 1e9
    best_board = [[0 for _ in range(board_size)] for _ in range(board_size)]
    if random.randint(0, 4) == 0:
        num_run = 0
    else:
        num_run = random.randint(50, random.randint(50, 1000))

    for t in range(0, num_run):
        num_moves = random.randint(0, 10)
        board = [[0 for _ in range(board_size)] for _ in range(board_size)]
        current_player = -1
        for _ in range(num_moves):
            # éšæœºé€‰æ‹©ä¸€ä¸ªç©ºä½ä¸‹æ£‹
            i, j = perm[_]
            board[i][j] = current_player
            current_player = -current_player
        if evaluation_func(board) != 0:
            continue

        value, policy = get_calc(model, board)
        val = max(float(value), -float(value))
        if val < best_val:
            best_val = val
            best_board = board
    #if Config.output_info:
    #    print(best_val)
    return best_board

class MCTSNode:
    def __init__(self, board, parent=None, move=None):
        self.board = board
        self.parent = parent
        self.move = move
        self.children = {}
        self.visit_count = 0
        self.value_sum = 0
        self.val = 0
        self.value = None
    
    def update_value(self):
        if len(self.children) == 0:
            self.val = self.value
        else:
            if Config.mcts_type == 'mean':
                self.val = self.value_sum / self.visit_count
            else:
                vals = max((-child.val if child != None else -10.) for child, prior in self.children.values())
                self.val = self.value if vals == -10 else vals

accumulate_sum = 0
class MCTS:
    def __init__(self, model, c_puct=0.8, puct2=0.02, parallel=0, use_rand=0.01):
        self.c_puct = c_puct
        self.puct2 = puct2
        self.use_rand = use_rand
        #self.device = 'cpu'
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
        elif torch.backends.mps.is_available():
            self.device = torch.device('mps')
        else:
            self.device = torch.device('cpu')
        if type(model) == str:
            self.model = ValueCNN()
            self.model.load_state_dict(torch.load(model,map_location=torch.device(self.device),weights_only=True))
        else:
            self.model = model.to(self.device)
        self.visited_nodes = []
    
    def new_node(self, *args, **kwargs):
        node = MCTSNode(*args, **kwargs)
        if Config.collect_subnode:
            self.visited_nodes.append(node)
        return node

    def board_to_key(self, board):
        return str(board)  # æˆ–ä½¿ç”¨æ›´é«˜æ•ˆçš„å“ˆå¸Œæ–¹æ³•

    def no_child(self, board):
        for i in range(0, board_size):
            for j in range(0, board_size):
                if board[i][j] == 0:
                    return False
        return True

    def is_terminal(self, board):
        if self.no_child(board):
            return True
        return evaluation_func(board) != 0

    def run(self, root_board, num_simulations, train = 0, cur_root = None, return_root = 0):
        global accumulate_sum
        accumulate_sum = 0
        all_beg = time.time_ns()

        if cur_root == None:
            root = MCTSNode(root_board)
            self.visited_nodes.append(root)
        else:
            root = cur_root
        
        for _ in range(num_simulations):
            node = root
            search_path = [node]
            
            while node.children:
                node = self.select_child(node)
                search_path.append(node)
            
            if not self.is_terminal(node.board):
                self.expand_node(node)
            else:
                node.value = self.evaluate_node(node)
            
            value = self.evaluate_node(node)
            
            for node in reversed(search_path):
                node.visit_count += 1
                node.value_sum += value
                node.update_value()
                value = -value
            #print(len(search_path), end=" ")
        
        #print("sum = ", accumulate_sum, ", total = ", time.time_ns() - all_beg, ", ratio = ", accumulate_sum / (time.time_ns() - all_beg))
        if not return_root:
            return self.get_results(root,train=train)
        else:
            return self.get_results(root,train=train), root

    
    def select_child(self, node : MCTSNode):
        global accumulate_sum
        beg = time.time_ns()
        total_visits = sum((child.visit_count if child != None else 0) for child, prior in node.children.values())
        explore_buff = math.pow(total_visits + 1, 0.5)
        log_total = math.log(total_visits + 1)
        
        best_score = -1e9
        best_move = None
        
        exp1 = 0
        exp2 = 0
        for child, prior in node.children.values():
            if child != None:
                exp1 += child.val * child.visit_count
                exp2 += child.visit_count
        ave = exp1 / (exp2 + 1e-5)

        #print(len(node.children.items()))

        tmp = 0
        for move, (child, prior) in node.children.items():
            explore = self.c_puct * prior * explore_buff
            exploit = ave
            if child != None and child.visit_count != 0:
                exploit = child.val
                explore /= (child.visit_count + 1)
            
            explore += self.puct2 * math.sqrt(log_total / ((child.visit_count if child else 0) + 1))
            score = explore - exploit
            if score > best_score:
                best_score = score
                best_move = move
        accumulate_sum += time.time_ns() - beg
        #print(type(best_score))
        
        
        chd, pri = node.children[best_move]
        if chd == None:
            i, j = best_move
            new_board = copy.deepcopy(node.board)
            new_board[i][j] = 1 
            for x in range(board_size):
                for y in range(board_size):
                    new_board[x][y] *= -1
            chd = MCTSNode(new_board, parent=node, move=best_move)
            if Config.collect_subnode:
                self.visited_nodes.append(chd)
            node.children[best_move] = chd, pri
        return chd

    def expand_node(self, node : MCTSNode):
        global accumulate_sum
        tm_beg = time.time_ns()
        
        node.value, policy = get_calc(self.model, node.board)

        sum_1 = 0
        for i in range(board_size):
            for j in range(board_size):
                if node.board[i][j] == 0:
                    sum_1 += policy[i][j]
        if sum_1 == 0:
            sum_1 += 1e-10
        for i in range(board_size):
            for j in range(board_size):
                if node.board[i][j] == 0:  
                    node.children[(i, j)] = None, policy[i][j] / sum_1 + random.normalvariate(mu=0, sigma=self.use_rand)
        accumulate_sum += time.time_ns() - tm_beg
    
    def evaluate_board(self, board):
        global accumulate_sum
        if self.no_child(board):
            return 0
        eval = evaluation_func(board)
        if eval != 0:
            return eval
        tm_beg = time.time_ns()
        value, _ = get_calc(self.model, board)
        accumulate_sum += time.time_ns() - tm_beg
        return value

    def evaluate_node(self, node : MCTSNode):
        if self.no_child(node.board):
            return 0
        eval = evaluation_func(node.board)
        if eval != 0:
            return eval
        if node.value == None:
            assert False
        return node.value
    def get_results(self, root : MCTSNode, train=0):
        probs = np.zeros((board_size, board_size))
        total_visits = sum((child.visit_count if child != None else 0) for child, prior in root.children.values())
        
        if not train:
            for move, (child, prior) in root.children.items():
                if child != None:
                    i, j = move
                    probs[i, j] = child.visit_count / total_visits if total_visits > 0 else 0
            return root.value_sum / root.visit_count, probs
        else:
            org_value, org_policy = get_calc(self.model, root.board)
            good = 0
            for i in range(0, board_size):
                for j in range(0, board_size):
                    if root.board[i][j] == 0:
                        good += org_policy[i][j]
            for i in range(0, board_size):
                for j in range(0, board_size):
                    if root.board[i][j] != 0 or org_policy[i][j] == 0:
                        probs[i, j] = 0
                    else:
                        probs[i, j] = org_policy[i][j] / good
            
            sum_used = 0
            moves = []
            for move, (child, prior) in root.children.items():
                if child != None and child.visit_count != 0:
                    sum_used += probs[move]
                    probs[move] = 0
                    moves.append((move[0], move[1], child))
            moves.sort(key=lambda x: x[2].visit_count, reverse=True)
            value_sum = 0
            for i in range(0, len(moves)):
                cnt = moves[i][2].visit_count
                if i + 1 < len(moves):
                    cnt -= moves[i + 1][2].visit_count
                if cnt == 0:
                    continue
                cur = -1e9
                best_pos = i
                for j in range(0, i + 1):
                    vals = -moves[j][2].val
                    if vals > cur:
                        cur = vals
                        best_pos = j
                probs[moves[best_pos][0], moves[best_pos][1]] += sum_used * cnt * (i + 1) / total_visits
                value_sum += cur * cnt * (i + 1) / total_visits
            for i in range(0, board_size):
                for j in range(0, board_size):
                    if probs[i, j] < 0:
                        print(good)
                        print(root.board[i][j])
                        assert False
            return value_sum, probs
    
    def get_train_data(self):
        boards, policies, values, weights = [], [], [], []
        for root in self.visited_nodes:
            child_count = sum((1 if child != None else 0) for child, prior in root.children.values())
            if not (child_count > 1 or root.visit_count >= Config.train_simulation / 2):
                continue
            #root : MCTSNode
            
            probs = np.zeros((board_size, board_size))
            total_visits = sum((child.visit_count if child != None else 0) for child, prior in root.children.values())
            org_value, org_policy = get_calc(self.model, root.board)
            good = 0
            for i in range(0, board_size):
                for j in range(0, board_size):
                    if root.board[i][j] == 0:
                        good += org_policy[i][j]
            for i in range(0, board_size):
                for j in range(0, board_size):
                    if root.board[i][j] != 0 or org_policy[i][j] == 0:
                        probs[i, j] = 0
                    else:
                        org_policy[i][j] /= good
                        probs[i, j] = org_policy[i][j]
            
            sum_used = 0
            moves = []
            for move, (child, prior) in root.children.items():
                if child != None and child.visit_count != 0:
                    sum_used += probs[move]
                    moves.append((move[0], move[1], child, probs[move]))
                    probs[move] = 0
            moves.sort(key=lambda x: x[3], reverse=True)
            value_sum = 0
            for i in range(0, len(moves)):
                cnt = moves[i][3]
                if i + 1 < len(moves):
                    cnt -= moves[i + 1][3]
                if cnt == 0:
                    continue
                cur = -1e9
                best_pos = i
                for j in range(0, i + 1):
                    vals = -moves[j][2].val
                    if vals > cur:
                        cur = vals
                        best_pos = j
                probs[moves[best_pos][0], moves[best_pos][1]] += cnt * (i + 1)
                value_sum += cur * cnt * (i + 1) / sum_used
            for i in range(0, board_size):
                for j in range(0, board_size):
                    if probs[i, j] < 0:
                        print(good)
                        print(root.board[i][j])
                        assert False
            boards.append(board_to_tensor(copy.deepcopy(root.board)))
            policies.append(torch.FloatTensor(probs))
            values.append(value_sum)
            weights.append(math.sqrt(total_visits / Config.train_simulation) * Config.train_buff)
        return boards, policies, values, weights
    
    def show_data(self, root):
        visit_matrix = np.zeros((board_size, board_size), dtype=np.int32)
        value_matrix = np.zeros((board_size, board_size), dtype=np.float32)

        for move, (child, _) in root.children.items():
            if child is not None:
                i, j = move
                visit_matrix[i][j] = child.visit_count
                value_matrix[i][j] = -child.val
        return root.val, visit_matrix, value_matrix

def show_nn(model, board):
    _, prob = get_calc(model, board)
    val = np.zeros((board_size, board_size), dtype=np.float32)
    for i in range(board_size):
        for j in range(board_size):
            if board[i][j] != 0:
                prob[i][j] = 0
            else:
                new_board = copy.deepcopy(board)
                new_board[i][j] = 1 
                for x in range(board_size):
                    for y in range(board_size):
                        new_board[x][y] *= -1
                val[i][j], _ = get_calc(model, new_board)
                val[i][j] = float(-val[i][j])
    return prob, val

def augment_data(boards, policies, values, weights):
    augmented_boards = []
    augmented_policies = []
    augmented_values = []
    augmented_weights = []
    
    for board, policy, value, weight in zip(boards, policies, values, weights):
        value = torch.tensor(value).clone().detach().float()
        weight = torch.tensor(weight).clone().detach().float()
        
        # D4 å¯¹ç§°å˜æ¢
        for k in range(4):
            for o in range(2):
                new_board = torch.rot90(board, k, [1, 2])
                new_policy = torch.rot90(policy, k, [0, 1])
                if o:
                    new_board = torch.flip(new_board, [2])
                    new_policy = torch.flip(new_policy, [1])
                
                augmented_boards.append(new_board)
                augmented_policies.append(new_policy)
                augmented_values.append(value)
                augmented_weights.append(weight)
    
    return (
        torch.stack(augmented_boards),
        torch.stack(augmented_policies),
        torch.stack(augmented_values),
        torch.stack(augmented_weights)
    )

def generate_selfplay_data(model, num_games, num_simulations=Config.train_simulation):
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨MPSï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼
    if torch.backends.mps.is_available():
        # å•è¿›ç¨‹æ¨¡å¼
        results = []
        model_state_dict = model.state_dict()
        
        for i in tqdm(range(num_games), desc="Generating games"):
            result = generate_single_game(i, model_state_dict, num_simulations)
            results.append(result)
    else:
        # ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œç”Ÿæˆæ¸¸æˆ
        model_state_dict = model.state_dict()
        with multiprocessing.get_context('spawn').Pool(
            processes=Config.num_workers
        ) as pool:
            func = partial(
                generate_single_game,
                model_state_dict=model_state_dict,
                num_simulations=num_simulations
            )
            results = list(tqdm(
                pool.imap(func, range(num_games)),
                total=num_games,
                desc="Generating games"
            ))

    # æ•´åˆç»“æœ
    boards, policies, values, weights = [], [], [], []
    for game_boards, game_policies, game_values, game_weights in results:
        boards.extend(game_boards)
        policies.extend(game_policies)
        values.extend(game_values)
        weights.extend(game_weights)
    
    boards = torch.stack(boards)
    policies = torch.stack(policies)
    values = torch.FloatTensor(values)
    weights = torch.FloatTensor(weights)
    
    # æ•°æ®å¢å¼ºï¼šæ—‹è½¬å’Œç¿»è½¬
    print("len_boards_raw = ", len(boards))
    boards, policies, values, weights = augment_data(boards, policies, values, weights)
    
    return boards, policies, values, weights

def calc_next_move(board, probs, temperature=0):
    valid_moves = []
    for i in range(board_size):
        for j in range(board_size):
            if board[i][j] == 0:
                valid_moves.append((i, j, probs[i][j]))
    if temperature == 0:
        valid_moves.sort(key=lambda x: x[2], reverse=True)
        return valid_moves[0][:2]
    else:
        moves = [(i, j) for i, j, _ in valid_moves]
        probs = np.array([p for _, _, p in valid_moves], dtype=np.float64)
        if temperature != 1.0:
            probs = probs ** (1.0 / temperature)
        probs = probs / probs.sum()
        chosen_index = np.random.choice(len(moves), p=probs)
        return moves[chosen_index]  


def generate_single_game(_, model_state_dict, num_simulations):
    """ç”Ÿæˆå•å±€æ¸¸æˆæ•°æ®"""
    model = ValueCNN()
    model.load_state_dict(model_state_dict)
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    board = generate_random_board(model)
    temperature=0.1*random.randint(0,9)
    
    with torch.no_grad():
        mcts = MCTS(model)
        
        game_values = []
        root = None
        for move_num in range(board_size*board_size):
            # MCTSè·å–ç­–ç•¥
            infos, new_root = mcts.run(board, num_simulations, train=0, cur_root=root, return_root=1)
            value, action_probs = infos
            
            # é€‰æ‹©åŠ¨ä½œ
            
            action = calc_next_move(board, action_probs, temperature)
            # æ‰§è¡ŒåŠ¨ä½œ
            board[action[0]][action[1]] = 1

            if new_root.children[action] != None:
                root, _ = new_root.children[action]
            else:
                root = None

            # æ£€æŸ¥æ¸¸æˆç»“æŸ
            if mcts.is_terminal(board):
                break
            
            # ç¿»è½¬è§†è§’
            for i in range(board_size):
                for j in range(board_size):
                    board[i][j] *= -1
            #print(action[0], action[1])

        #print(len(game_values), game_values[len(game_values) - 1])
    
        if Config.output_info:
            print(move_num)
        return mcts.get_train_data()

class Model:
    def __init__(self, location, use_rand=0.01,simulations=200, c_puct=1):
        self.simulations=simulations
        self.model = ValueCNN()
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
        elif torch.backends.mps.is_available():
            self.device = torch.device('mps')
        else:
            self.device = torch.device('cpu')
        self.model.load_state_dict(torch.load(location,map_location=torch.device(self.device),weights_only=True))
        self.mcts = MCTS(self.model,use_rand=use_rand,c_puct=c_puct)
    def call(self, board, temperature=0, simulations=-1,debug=0):
        if simulations == -1:
            simulations = self.simulations
        _, action_probs = self.mcts.run(copy.deepcopy(board), simulations)
        if debug != 0:
            print("expected_value = ", _)
            print("Probabilty = ")
            for i in range(0, board_size):
                for j in range(0, board_size):
                    if board[i][j] == 1:
                        print("  o", end=" ")
                    elif board[i][j] == 0:
                        print("  x", end=" ")
                    else:
                        print(f"{action_probs[i][j]:2f}", end=" ")
                print('\n')

        return calc_next_move(board, action_probs, temperature)



def train_model(model, train_loader, val_loader, config):
    """è®­ç»ƒæ¨¡å‹"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
    else:
        device = torch.device('cpu')
    model.to(device)
    
    value_criterion = nn.MSELoss(reduction='none')
    policy_criterion = nn.KLDivLoss(reduction='none')

    val_value_criterion = nn.MSELoss()
    val_policy_criterion = nn.KLDivLoss(reduction='batchmean')

    
    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 'min', patience=2, factor=0.5)
    
    train_losses = []
    val_losses = []
    
    print(f"å¼€å§‹è®­ç»ƒï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    
    for epoch in range(config.num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_value_loss, train_policy_loss = 0, 0
        
        for batch_boards, batch_policies, batch_values, batch_weights in tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.num_epochs}'):
            batch_boards = batch_boards.to(device)
            batch_policies = batch_policies.to(device).view(batch_policies.size(0), -1)
            batch_values = batch_values.to(device).unsqueeze(1)  # æ·»åŠ ç»´åº¦åŒ¹é…
            batch_weights = batch_weights.to(device)
            
            optimizer.zero_grad()
            
            pred_values, pred_policies = model(batch_boards)
            
            # è®¡ç®—æŸå¤±
            value_loss = value_criterion(pred_values, batch_values).squeeze(1)
            policy_loss = policy_criterion(F.log_softmax(pred_policies, dim=1),
                batch_policies.view(-1, batch_policies.size(-1))).sum(dim=1,keepdim=True)
            #print(value_loss.shape)
            #print(policy_loss.shape)
            #print(pred_values.shape)
            #print(pred_policies.shape)

            weighted_value_loss = (value_loss * batch_weights).mean()      
            weighted_policy_loss = (policy_loss * batch_weights).mean()

            loss = 2 * weighted_value_loss + weighted_policy_loss
            
            loss.backward()
            optimizer.step()
            
            train_value_loss += weighted_value_loss.item()
            train_policy_loss += weighted_policy_loss.item()

        
        model.eval()
        val_value_loss, val_policy_loss = 0, 0
        with torch.no_grad():
            for boards, policies, values, weights in val_loader:
                boards = boards.to(device)
                policies = policies.to(device).view(policies.size(0), -1)
                values = values.to(device).unsqueeze(1)
                
                pred_values, pred_policies = model(boards)
                val_value_loss += val_value_criterion(pred_values, values).item()
                val_policy_loss += val_policy_criterion(
                    F.log_softmax(pred_policies, dim=1),
                    policies.view(-1, policies.size(-1))  
                ).item()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_train_value = train_value_loss / len(train_loader)
        avg_train_policy = train_policy_loss / len(train_loader)
        avg_val_value = val_value_loss / len(val_loader)
        avg_val_policy = val_policy_loss / len(val_loader)

        print("Train: ", train_value_loss, train_policy_loss)
        print("Val: ", val_value_loss, val_policy_loss)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        val_total_loss = 2 * avg_val_value + avg_val_policy
        scheduler.step(val_total_loss)

        train_losses.append(avg_train_value + avg_train_policy)
        val_losses.append(avg_val_value + avg_val_policy)
    
    return train_losses, val_losses


def plot_training_history(train_losses, val_losses):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training History')
    plt.legend()
    plt.grid(True)
    plt.show()

# ä¸»è®­ç»ƒå‡½æ•°
def work():
    """
    ä¸»è®­ç»ƒå‡½æ•° - 50è½®å¼ºåŒ–è®­ç»ƒï¼Œæå‡æ¨¡å‹æ£‹åŠ›æ¥è¿‘4090è®­ç»ƒæ°´å¹³
    æ”¯æŒæ–­ç‚¹ç»­è®­åŠŸèƒ½
    """
    import time
    import glob
    start_time = time.time()
    
    config = Config()
    model = ValueCNN()
    
    # ğŸ”„ æ£€æŸ¥æ˜¯å¦æœ‰å·²è®­ç»ƒçš„æ¨¡å‹ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰
    start_round = 0
    if os.path.exists(config.model_path):
        existing_models = glob.glob(os.path.join(config.model_path, '[0-9]*.pth'))
        if existing_models:
            # æ‰¾åˆ°æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
            model_numbers = []
            for model_file in existing_models:
                try:
                    num = int(os.path.basename(model_file).split('.')[0])
                    model_numbers.append(num)
                except:
                    continue
            
            if model_numbers:
                latest_round = max(model_numbers)
                latest_model_path = os.path.join(config.model_path, f"{latest_round}.pth")
                
                print(f"ğŸ”„ å‘ç°å·²è®­ç»ƒæ¨¡å‹ï¼Œä»ç¬¬{latest_round}è½®ç»§ç»­è®­ç»ƒ")
                print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {latest_model_path}")
                model.load_state_dict(torch.load(latest_model_path, weights_only=True))
                start_round = latest_round
                print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå°†ä»ç¬¬{start_round + 1}è½®å¼€å§‹ç»§ç»­è®­ç»ƒ")
    
    if config.base_path != None and start_round == 0:
        model.load_state_dict(torch.load(config.base_path, weights_only=True))
        print(f"ğŸ“‚ ä»æŒ‡å®šè·¯å¾„åŠ è½½åˆå§‹æ¨¡å‹: {config.base_path}")
    
    if start_round == 0:
        print("ğŸš€ å¼€å§‹50è½®å¼ºåŒ–è®­ç»ƒè®¡åˆ’")
    else:
        print(f"ğŸ”„ ç»§ç»­50è½®å¼ºåŒ–è®­ç»ƒè®¡åˆ’ (ä»ç¬¬{start_round + 1}è½®å¼€å§‹)")
    
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   - æ€»è®­ç»ƒè½®æ•°: 50")
    print(f"   - å·²å®Œæˆè½®æ•°: {start_round}")
    print(f"   - å‰©ä½™è½®æ•°: {50 - start_round}")
    print(f"   - æ¯è½®æ ·æœ¬æ•°: {config.num_samples}")
    print(f"   - æ¯è½®epochs: {config.num_epochs}")
    print(f"   - MCTSæ¨¡æ‹Ÿæ¬¡æ•°: {config.train_simulation}")
    print(f"   - æ‰¹å¤„ç†å¤§å°: {config.batch_size}")
    print(f"   - æ¨¡å‹é€šé“æ•°: {config.channel}")
    print(f"   - é¢„è®¡æ€»æ ·æœ¬: {50 * config.num_samples}")
    print("="*60)
    
    for t in range(start_round, 50):  # ä»æ–­ç‚¹å¼€å§‹ç»§ç»­è®­ç»ƒ
        round_start = time.time()
        model.eval()
        print(f"ğŸ”„ ç¬¬{t+1}/50è½®è®­ç»ƒå¼€å§‹...")
        print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%H:%M:%S')}")
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        print("ğŸ“ˆ ç”Ÿæˆè‡ªåšå¼ˆæ•°æ®...")
        boards, policies, values, weights = generate_selfplay_data(model, config.num_samples)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        num_train = int(len(boards) * config.train_ratio)
        train_boards = boards[:num_train]
        train_policies = policies[:num_train]
        train_values = values[:num_train]
        train_weights = weights[:num_train]

        val_boards = boards[num_train:]
        val_policies = policies[num_train:]
        val_values = values[num_train:]
        val_weights = weights[num_train:]
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = Weighted_Dataset(train_boards, train_policies, train_values, train_weights)
        val_dataset = Weighted_Dataset(val_boards, val_policies, val_values, val_weights)
        
        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)
        
        # è®­ç»ƒæ¨¡å‹
        print("ğŸ§  å¼€å§‹ç¥ç»ç½‘ç»œè®­ç»ƒ...")
        train_losses, val_losses = train_model(model, train_loader, val_loader, config)
        
        round_time = time.time() - round_start
        elapsed_time = time.time() - start_time
        completed_rounds = t - start_round + 1
        avg_time_per_round = elapsed_time / completed_rounds
        remaining_rounds = 50 - t - 1
        estimated_remaining = avg_time_per_round * remaining_rounds
        
        # ç»˜åˆ¶è®­ç»ƒå†å²
        #plot_training_history(train_losses, val_losses)
        print(f"âœ… ç¬¬{t+1}/50è½®è®­ç»ƒå®Œæˆ")
        print(f"â±ï¸  æœ¬è½®è€—æ—¶: {round_time/60:.1f}åˆ†é’Ÿ")
        print(f"âŒ› æœ¬æ¬¡è¿è¡Œæ€»è€—æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ")
        print(f"ğŸ“ˆ å®Œæˆè¿›åº¦: {(t+1)/50*100:.1f}% ({t+1}/50)")
        if completed_rounds > 1:
            print(f"ğŸ”® é¢„è®¡å‰©ä½™: {estimated_remaining/60:.1f}åˆ†é’Ÿ")
        print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {[f'{loss:.4f}' for loss in train_losses]}")
        print(f"ğŸ“‰ éªŒè¯æŸå¤±: {[f'{loss:.4f}' for loss in val_losses]}")
        
        # ä¿å­˜æ¨¡å‹
        os.makedirs(config.model_path, exist_ok=True) 
        checkpoint = os.path.join(config.model_path, f"{t + 1}.pth")
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹: {checkpoint}")
        torch.save(model.state_dict(), checkpoint)
        print(f"âœ… ç¬¬{t+1}è½®è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜")
        
        # æ¯10è½®ä¿å­˜ä¸€ä¸ªå¤‡ä»½
        if (t + 1) % 10 == 0:
            backup_path = os.path.join(config.model_path, f"backup_round_{t + 1}.pth")
            torch.save(model.state_dict(), backup_path)
            print(f"ğŸ”„ åˆ›å»ºç¬¬{t+1}è½®å¤‡ä»½: {backup_path}")
        
        print("="*60)
    
    total_time = time.time() - start_time
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼50è½®å¼ºåŒ–è®­ç»ƒå·²ç»“æŸ")
    print(f"ğŸ† æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f}å°æ—¶")
    print(f"ğŸ“ˆ å¹³å‡æ¯è½®: {total_time/50/60:.1f}åˆ†é’Ÿ")
    return model

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    work()
